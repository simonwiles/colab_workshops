{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "a2O6ZbjYEuCv",
    "outputId": "852debeb-963f-4e53-b8d3-2ee66253efbc"
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade --quiet gspread\n",
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 139
    },
    "colab_type": "code",
    "id": "4eZqUZJxjAga",
    "outputId": "3531c43a-a78d-4ea8-eb12-958ddf3ea9e4"
   },
   "outputs": [],
   "source": [
    "#Import necessary libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import urllib.request\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sb\n",
    "from scipy.stats import linregress\n",
    "from scipy.stats import ttest_ind\n",
    "from scipy.stats import ttest_rel\n",
    "import re\n",
    "import requests\n",
    "import nltk\n",
    "import string\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer as sentVader\n",
    "nltk.download('vader_lexicon')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KbX6qLISjETC"
   },
   "outputs": [],
   "source": [
    "#Helper Functions and data\n",
    "\n",
    "def get_trump_tweets():\n",
    "  # This function grabs the Trump tweets from the CTASS GitHub\n",
    "  return pd.read_json('https://raw.githubusercontent.com/ctass/fall-workshop-2019/master/trump_tweets.json')\n",
    "\n",
    "party_name_dict = pd.read_json('https://raw.githubusercontent.com/ctass/fall-workshop-2019/master/moc_handle_dict.json')\n",
    "\n",
    "def get_cong_tweet(date):\n",
    "  #This function takes a date such as '2018-12-25' as a parameter and outputs\n",
    "  #the tweets of all of the members of congress' tweets on that day. We combine\n",
    "  #this data with a simple dictionary that contains MoCs' twitter handles,\n",
    "  #names, and party affiliation.\n",
    "  url = 'https://raw.githubusercontent.com/alexlitel/congresstweets/master/data/{}.json'.format(date)\n",
    "  tweets = pd.read_json(url)\n",
    "  global party_name_dict\n",
    "  return tweets.merge(party_name_dict, on='screen_name')\n",
    "\n",
    "def get_mfd():\n",
    "  #This function grabs a version of the moral foundations dictionary from the\n",
    "  #CTASS GitHub which can be used with the dict_vec function below\n",
    "  return requests.get('https://raw.githubusercontent.com/ctass/fall-workshop-2019/master/mfd.dict').json()\n",
    "\n",
    "def get_news(source, date):\n",
    "    #This function grabs the front page of MSNBC or Fox News on a particular day\n",
    "    #returns it as a single string. These were sourced from the Internet Archive \n",
    "    #and stored on the CTASS Github. The function takes a source (either 'msnbc'\n",
    "    #or 'fox') and dates of the form YYYY-MM-DD. News has been \n",
    "    #archived from '2019-10-21' to '2019-11-21' (inclusive), except that for a\n",
    "    #couple of days we had to grab a post near midnight the next day.\n",
    "    base = 'https://raw.githubusercontent.com/ctass/fall-workshop-2019/master/news/'\n",
    "    url = base + source + '-' + date.replace('-', '') + '.txt'\n",
    "    data = urllib.request.urlopen(url)\n",
    "    return data.read().decode('utf-8').replace('\\n', ' ').lower()\n",
    "    \n",
    "def regexify(s):\n",
    "  #Helper function for dictify below\n",
    "  if s[-1] == \"*\":\n",
    "    return r'(?<!\\w){}\\w*?[\\s.]'.format(s[:-1])\n",
    "  return r'(?<!\\w){}[\\s.]'.format(s)\n",
    "\n",
    "def dictify(d):\n",
    "  #Takes in a dictionary where keys are categories and the values are lists of\n",
    "  #words that indicate each category. Outputs a dictionary that can be used by\n",
    "  #dict_vec below. Helpful if you are not comfortable with regular expressions,\n",
    "  #but is limited by design. There is one key functionality built in: if a word\n",
    "  #ends in a \"*\", then the generated regular expression will look for words with\n",
    "  #any additional letters at the end of it (e.g. \"eat*\" captures \"eat\", \n",
    "  #\"eating\", \"eats\", etc.). The same does not hold for \"*\" at the beginning of a \n",
    "  #word.\n",
    "  return {k: [regexify(w) for w in d[k]] for k in d}\n",
    "\n",
    "def dict_vec(s, d):\n",
    "  #Takes in a string and a dictionary which is structured {C1: [W11, W21, ...],\n",
    "  #                                                        C2: [W12, W22, ...]}\n",
    "  #Where Cx is the name of a category and Wxy is a regular expression which\n",
    "  #indicates a string 'more belongs' in category x and outputs a dictionary\n",
    "  #structured {C1: S1, C2: S2, ...} where Cx is the name of a category and\n",
    "  #Sx is how much the passed string 'belongs' in that category.\n",
    "  results = {k: 0 for k in d.keys()}\n",
    "  results['_total_'] = 0\n",
    "  s = s.lower().translate(str.maketrans('', '', string.punctuation))\n",
    "  for k in d.keys():\n",
    "    for t in d[k]:\n",
    "      count = len(re.findall(t,s.lower()+' '))\n",
    "      results[k] += count\n",
    "      results['_total_'] += count\n",
    "  results['_len_'] = len(nltk.tokenize.word_tokenize(s))\n",
    "  return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tPxTi3f-ZtWp"
   },
   "source": [
    "#**Accessing Three Available Corpora**\n",
    "\n",
    "For the purposes of this workshop we've made starter code that allows you to easily access three different text corpora: (1) DJT's tweets, (2) Tweets of every member of the US congress, and (3) the front pages of MSNBC.com and FoxNews.com. We've attempted to make the starter code as straight-forward as possible, and below provide code that you can copy and paste for many analyses. **DISCLAIMER:** all of the corpora come from second-hand sources. Therefore, we cannot speak to the accuracy of these data, though from some intial snooping around the data seems to check out."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L0kTqSKRZIuk"
   },
   "source": [
    "##Donald Trump's Tweets since the Inauguration\n",
    "We have a little helper function that allows you to download DJT's tweets from\n",
    "the CTASS GitHub. It will automatically output it as a Pandas DataFrame,\n",
    "which is a useful tool for doing data science in Python. See below for an\n",
    "example of loading the tweets and assigning it to a variable df. We also\n",
    "remove retweets (you don't have to) and drop the id_str variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 419
    },
    "colab_type": "code",
    "id": "VL3MCrueOGdW",
    "outputId": "1fa17d4a-b908-46b4-a355-de95d9e1590b"
   },
   "outputs": [],
   "source": [
    "df = get_trump_tweets()\n",
    "df = df[df['is_retweet']==0.0].drop(['is_retweet','id_str'],axis=1)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pplwKopk14KB"
   },
   "source": [
    "##Tweets from Members of Congress (MoCs)\n",
    "Next we have a function that takes in a date (e.g. '2017-11-17') and spits back\n",
    "all the tweets sent out by any member of congress on that day. Importantly,\n",
    "this data is maintained by the GitHub user alexlitel, so our ability to access\n",
    "this data is all due to them. Thank you whoever you are! Importantly, we don't\n",
    "have access to the number of favorites or retweets of these tweets. Also, we only have access to tweets starting on June 21st of 2017. The function then also matches these Twitter handles to the MoC's name and their party affiliation, which combines data maintained by triagecancer.org, opendatasoft.com, and opensecrety.org. We'll drop some columns that don't on their face appear to be helpful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 589
    },
    "colab_type": "code",
    "id": "bMk_wr_Szouu",
    "outputId": "cd9d40bf-a7d9-40da-a7b5-6d5929c2ae8c"
   },
   "outputs": [],
   "source": [
    "get_cong_tweet('2017-11-17').drop(['id','user_id'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BCPPjsoiY5HZ"
   },
   "source": [
    "##The front pages of Foxnews.com and MSNBC.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "enelvnyUR0bd"
   },
   "source": [
    "Lastly, we have a small corpus we've collected (with the help of the Wayback machine internet archive) of the front page of msnbc.com and foxnews.com. We've collected all of the text from the front of these websites from October 21st to November 21st, with the one caveat that for a couple of days (November 7th and November 10th) we couldn't grab the front page of MSNBC, so we instead use a grab from around midnight the next day. The function takes in the source (either 'msnbc' or 'fox') and the date (e.g. '2019-11-19' for November 11th of 2019) and returns back a string that isn't that pretty but contains all the relevant information. Since we're primarily exploring dictionary methods here this won't be too much of a problem, but you could imagine that if we wanted to use more complicated methods that leveraged the structure of language we might need to be a little more sophisticated with how we collect and clean this data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "SufstokTY4fN",
    "outputId": "9b4dd5ae-2111-44cc-9735-4d75311fa0f1"
   },
   "outputs": [],
   "source": [
    "get_news('msnbc','2019-11-19')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "92TgJgZHZ8CJ"
   },
   "source": [
    "#**Example Analyses Using the Data**\n",
    "Next, we do some basic data analysis and visualization. If you're not very comfortable doing analyses in Python, copying and pasting this code and changing a few variables here and there is a great way to get started. Regardless of your comfort level with Python, these are some fairly simple analyses that can get your creativity going to come up with your own analyses. There's plenty of different ways we could have done any of these analyses; we emphasized simplicity of code and analyses that are easy to follow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gbusTeOvf6ax"
   },
   "source": [
    "##Basic Data Visualization and Analysis in Colab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Bd_B35ZulqGT"
   },
   "source": [
    "We're going to use a library called Seaborn for visualization. There are many\n",
    "other tools that are worth looking into, but Austin likes Seaborn because\n",
    "it's pretty easy to use, especially if you're already familiar with matplot,\n",
    "and makes reasonably pretty graphs (big step up from standard Stata graphs at least). This command just sets the style of our graphs. Austin likes this particular style, but you can also try 'darkgrid', 'dark', 'white', or 'ticks'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lma--ABG0q8l"
   },
   "outputs": [],
   "source": [
    "sb.set_style('whitegrid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "B6TGGq0plvyd"
   },
   "source": [
    "Here we'll produce a regression plot of the relationship between the\n",
    "number of retweets a tweet has and the number of favorites it has.\n",
    "We'll then do a regression using scipy to get a p-value (though we can see from\n",
    "the bootstrapped confidence intervals in the plot that this is relationship\n",
    "is highly significant). One thing that would definitely make this plot more\n",
    "legible is setting bounds on the x and y axis to exclude outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 329
    },
    "colab_type": "code",
    "id": "fR5UD9noh03s",
    "outputId": "eff54766-0766-4f2b-84c8-cadd4e7c3adf"
   },
   "outputs": [],
   "source": [
    "plt.scatter(df['retweet_count'],df['favorite_count'], alpha=0.25)\n",
    "#NOTE: \"alpha\" is a parameter that controls how transparent the scatterplot\n",
    "#points are in the scatterplot\n",
    "sb.regplot(df['retweet_count'],df['favorite_count'],scatter=False)\n",
    "plt.xlabel('# of Retweets')\n",
    "plt.ylabel('# of Favorites')\n",
    "plt.title(\"The relationship between RTs and favorites for DJT's tweets.\")\n",
    "plt.show()\n",
    "print('Does the number of favorites increase with the number of retweets?')\n",
    "print(linregress(df['retweet_count'],df['favorite_count']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CspHzl9Gl1fL"
   },
   "source": [
    "Here we'll look at the mean number of retweets and favorites for tweets\n",
    "from different sources. You can get a sense from the confidence intervals\n",
    "which tweet sources are more common in the data. I would consider dropping\n",
    "the Twitter Web Client category from this analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 346
    },
    "colab_type": "code",
    "id": "ObvRaNurkNo2",
    "outputId": "92b09a23-6400-4893-c2f9-0741b484435a"
   },
   "outputs": [],
   "source": [
    "mean_retweets = df.groupby('source')['retweet_count'].mean()\n",
    "std_retweets = df.groupby('source')['retweet_count'].std()\n",
    "\n",
    "mean_favorites = df.groupby('source')['favorite_count'].mean()\n",
    "std_favorites = df.groupby('source')['favorite_count'].std()\n",
    "\n",
    "source_rt_name, source_rt_code = np.unique(mean_retweets.index, \n",
    "                                           return_inverse=True)\n",
    "source_fv_name, source_fv_code = np.unique(mean_favorites.index, \n",
    "                                           return_inverse=True)\n",
    "\n",
    "plt.errorbar(source_rt_code, mean_retweets, xerr=0.2, yerr=2*std_retweets, \n",
    "             linestyle='', label='Retweets')\n",
    "plt.errorbar(source_rt_code, mean_favorites, xerr=0.2, yerr=2*std_retweets, \n",
    "             linestyle='', label='Favorites')\n",
    "\n",
    "plt.xticks(source_rt_code, source_rt_name, rotation=30, ha='right')\n",
    "plt.legend()\n",
    "plt.title(\"RTs and favorites by source of DJT's tweet.\")\n",
    "plt.xlabel('Source of tweet')\n",
    "plt.ylabel('Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "27sLOXcWl8F0"
   },
   "source": [
    "Here we're going to test whether Trump's tweets received more favorites and\n",
    "retweets on average in 2018 when compared to tweets in 2017. I excluded the\n",
    "2019 tweets because it makes the graphs look busy, but the command to plot them\n",
    "is simply commented out. Note that like many of our analyses here, our outcome is technically a count variable and should be analyzed by something that takes such a fact into account such as a negative binomial model, but also note from the histogram that it's incredibly unlikely that such a model would not return a significant association."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 641
    },
    "colab_type": "code",
    "id": "Z2gl6hZFt6X8",
    "outputId": "2b242353-2c40-4130-f1af-cf2d06c12d63"
   },
   "outputs": [],
   "source": [
    "rt = {2017: [],\n",
    "      2018: [],\n",
    "      2019: []}\n",
    "\n",
    "fv = {2017: [],\n",
    "      2018: [],\n",
    "      2019: []}\n",
    "\n",
    "for i, row in df.iterrows():\n",
    "    yr = row['created_at'].year\n",
    "    if row['retweet_count'] < 100000:\n",
    "      rt[yr].append(row['retweet_count']) \n",
    "    if row['favorite_count'] < 300000:\n",
    "      fv[yr].append(row['favorite_count'])\n",
    "\n",
    "sb.distplot(rt[2017], label='2017')\n",
    "sb.distplot(rt[2018], label='2018')\n",
    "#sb.distplot(rt[2019], label='2019')\n",
    "\n",
    "plt.legend()\n",
    "plt.xlabel('# of favorites')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Number of retweets by year.')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print(\"Is there a mean difference in the number of retweets for the years 2017 and 2018?\")\n",
    "print(ttest_ind(rt[2017],rt[2018],equal_var=False))\n",
    "\n",
    "sb.distplot(fv[2017], label='2017')\n",
    "sb.distplot(fv[2018], label='2018')\n",
    "#sb.distplot(fv[2019], label='2019')\n",
    "\n",
    "plt.legend()\n",
    "plt.xlabel('# of favorites')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Number of favorites by year.')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print(\"Is there a mean difference in the number of favorites for the years 2017 and 2018?\")\n",
    "print(ttest_ind(fv[2017],fv[2018],equal_var=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VvtAXLwA3Zmo"
   },
   "source": [
    "##Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yjGu5utRmKv3"
   },
   "source": [
    "This is a quick demo of how to use Vader, which is a sentiment analysis tool\n",
    "developed by the NLTK group here at Stanford. It's basically a sentiment\n",
    "lexicon, but uses some smart heuristics to do a slightly better job.\n",
    "You can get degree to which a string is positive, neutral, or negative, or you\n",
    "get a compound or summary score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "qV4_0o3O4-LB",
    "outputId": "4fd73072-e24d-4f09-ce96-cbacc751c737"
   },
   "outputs": [],
   "source": [
    "sent = sentVader()\n",
    "\n",
    "print(sent.polarity_scores('Everything about this is awesome. Except for the bagels, which are amazing'))\n",
    "print(sent.polarity_scores('Everything about this is terrible. Except for the bagels, which are amazing'))\n",
    "print(sent.polarity_scores('Everything about this is terrible. Except for the bagels, which are still not amazing'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "67H2nb-1mPQv"
   },
   "source": [
    "What can we do with sentiment analysis? Let's see whether Democrats and\n",
    "Republicans tweeted differently sentimented language this Wednesday, November 20th. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NeXZnf6vn2lY"
   },
   "outputs": [],
   "source": [
    "df = get_cong_tweet('2019-11-20')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hPlYSaW4n2zW"
   },
   "source": [
    "Then, we'll, create two lists; corresponding to the sentiments of tweets\n",
    "tweeted by Democrats and Republicans, respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "K9TzJKSDn3XA"
   },
   "outputs": [],
   "source": [
    "dems = []\n",
    "pubs = []\n",
    "sent = sentVader()\n",
    "\n",
    "for i, row in df.iterrows():\n",
    "  s = sent.polarity_scores(row['text'])['compound']\n",
    "  if row['party'] == 'Democrat':\n",
    "    dems.append(s)\n",
    "  elif row['party'] == 'Republican':\n",
    "    pubs.append(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dHykn8aWn3k2"
   },
   "source": [
    "Lastly, we'll produce a graph showing the histograms of sentiments by the two\n",
    "parties and do a simple t-test to asses whether there is a statistically\n",
    "significant difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 312
    },
    "colab_type": "code",
    "id": "r_ck65IeWS8m",
    "outputId": "6ef98cda-98a2-4f9a-d236-c732f54fa993"
   },
   "outputs": [],
   "source": [
    "sb.distplot(dems, label='Democrats', color='skyblue')\n",
    "sb.distplot(pubs, label='Republicans', color='crimson')\n",
    "\n",
    "plt.legend()\n",
    "plt.xlabel('Sentiment of tweet')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Sentiment of tweets this Wednesday.')\n",
    "\n",
    "print(ttest_ind(dems, pubs, equal_var=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "r_ucXnI0QOe_"
   },
   "source": [
    "Just as a cautionary note: often with computational methods you'll get crazy distributions like this, which don't necessarily meet the assumptions of different statistical tests such as a t-test, making your p-values invalid. It's nice to have a good understanding of statistics so that you know when you're breaking these rules, and how you can do analyses that don't make such assumptions. For instance, here (and in many other analyses we do here) you could do a Fischer randomization test, and specifically here the test statistic could be the mean of the sentiment score. The Fischer randomization test is model-free and doesn't make any assumption about the marginal distributions of the outcome variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cmuzH9Qb3b0B"
   },
   "source": [
    "##(Other) Dictionary Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qudlXLtNmZ42"
   },
   "source": [
    "Now we've got a series of functions to help you do other dictionary-\n",
    "based analyses. First up is this get_mfd function, which downloads a Python-\n",
    "friendly version of the Moral Foundations Dictionary from the CTASS GitHub.\n",
    "The dictionary isn't made up words per se, but instead regular expressions,\n",
    "which allow for efficient searching of resonably complicated patterns in text.\n",
    "There are other ways we could have coded this, but we preferred this for its\n",
    "flexibility and its scalability (in terms of the length of analyzed texts)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "NsA-DSoM49lR",
    "outputId": "259adde2-4b4b-49c4-dcec-91c011cc348e"
   },
   "outputs": [],
   "source": [
    "mfd = get_mfd()\n",
    "mfd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Uwcck_iTmedK"
   },
   "source": [
    "The next function takes in any such dictionary (you can make your own! See \n",
    "below) and outputs a dictionary indicating a string's score on each of the\n",
    "dictionary's categories. It also creates a \\_total\\_ category (the sum of all the other categories) as well as a \\_len\\_ category (the number of words in the string), in case you want to normalize the scores of the other categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 238
    },
    "colab_type": "code",
    "id": "iuHcjDn6dqJK",
    "outputId": "5195336f-3830-40a5-f563-e4539935ba6e"
   },
   "outputs": [],
   "source": [
    "dict_vec('This is an unjust test string. It is a very pure test string.',mfd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Wg-3jM38mihW"
   },
   "source": [
    "Here's a demo of defining your own dictionary for analysis. Though you can\n",
    "certainly do this with one line of code, we'll break into steps so it's easier\n",
    "to follow. First, define lists of words that relate to the different categories\n",
    "in your dictionary. You can put a \"*\" at the end of a word to capture any word\n",
    "that starts with those letters. For instance, \"eat*\" will capture \"eat\",\n",
    "\"eating\", and \"eats\", where \"eat\" will only capture \"eat\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nK6bxCkKkCLh"
   },
   "outputs": [],
   "source": [
    "politic_words = ['politi*', 'congress*', 'president*', 'democrat', 'republican']\n",
    "food_words = ['tuna', 'pineapple', 'burger', 'din*', 'eat*']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xPDURcZCmqGE"
   },
   "source": [
    "Then define a dictionary in which the key is a name for each category in the\n",
    "dictionary and the value are the appropriate lists you defined before\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MRwdui1UmqXP"
   },
   "outputs": [],
   "source": [
    "new_dict = {'politics': politic_words, \n",
    "            'food': food_words}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3CZBOUohmqkW"
   },
   "source": [
    "Then lastly pass that dictionary into our dictify function, which just turns\n",
    "your words into regular expressions for you!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5FzgSFqTmqyi"
   },
   "outputs": [],
   "source": [
    "new_dict = dictify(new_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rWBwifDpm08U"
   },
   "source": [
    "Then you can use your dictionary just like how we used the MF dictionary!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "AzSa9-DEm1HK",
    "outputId": "e22081f3-74ff-4a64-94c3-2d77ec77625b"
   },
   "outputs": [],
   "source": [
    "dict_vec('These congressional hearings are eating him alive', new_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0ntKfHRhm7nh"
   },
   "source": [
    "So what can we do with these dictionary methods? Moral foundations theory\n",
    "posits that conservatives care more about a subset of moral dimensions (namely\n",
    "purity, authority, and ingroup) than liberals. Something that would be\n",
    "consistent with such a theory would be if FoxNews.com (which has a more\n",
    "conservative viewership) used more language that appealed to such moral\n",
    "foundations than MSNBC.com (which has a more liberal viewership). So, we'll\n",
    "simply compare the text on the front pages of these two websites and see the\n",
    "degree to which these two websites use words from those sections of the moral\n",
    "foundations dictionary more.\n",
    "We'll iterate over each day of data that we have. For each day, we'll load the\n",
    "text from the front pages of the two websites. For each website, we'll get the\n",
    "count of words in the vice and virtue categories associatied with ingroup,\n",
    "authority, and purity, and add that number normalized by the number of words on the front page to lists we maintain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cnqzH6uYaPd9"
   },
   "outputs": [],
   "source": [
    "msnbc_pur = []\n",
    "msnbc_auth = []\n",
    "msnbc_ingrp = []\n",
    "\n",
    "fox_pur = []\n",
    "fox_auth = []\n",
    "fox_ingrp = []\n",
    "\n",
    "for date in ['2019-10-21','2019-10-22','2019-10-23','2019-10-24','2019-10-25',\n",
    "             '2019-10-26','2019-10-27','2019-10-28','2019-10-29','2019-10-30',\n",
    "             '2019-10-31',\n",
    "             '2019-11-01','2019-11-02','2019-11-03','2019-11-04','2019-11-05',\n",
    "             '2019-11-06','2019-11-07','2019-11-08','2019-11-09','2019-11-10',\n",
    "             '2019-11-11','2019-11-12','2019-11-13','2019-11-14','2019-11-15',\n",
    "             '2019-11-16','2019-11-17','2019-11-18','2019-11-19','2019-11-20',\n",
    "             '2019-11-21']:\n",
    "\n",
    "  msnbc = get_news('msnbc',date)\n",
    "  msnbc_vec = dict_vec(msnbc, mfd)\n",
    "  m_len = msnbc_vec['_len_']\n",
    "\n",
    "  fox = get_news('fox',date)\n",
    "  fox_vec = dict_vec(fox, mfd)\n",
    "  f_len = fox_vec['_len_']\n",
    "\n",
    "  msnbc_pur.append((msnbc_vec['PurityVirtue'] + msnbc_vec['PurityVice'])/m_len)\n",
    "  msnbc_auth.append((msnbc_vec['AuthorityVirtue'] + msnbc_vec['AuthorityVice'])/m_len)\n",
    "  msnbc_ingrp.append((msnbc_vec['IngroupVirtue'] + msnbc_vec['IngroupVice'])/m_len)\n",
    "\n",
    "  fox_pur.append((fox_vec['PurityVirtue'] + fox_vec['PurityVice'])/f_len)\n",
    "  fox_auth.append((fox_vec['AuthorityVirtue'] + fox_vec['AuthorityVice'])/f_len)\n",
    "  fox_ingrp.append((fox_vec['IngroupVirtue'] + fox_vec['IngroupVice'])/f_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vWrYsh5dnLlw"
   },
   "source": [
    "Now, we're going to make a weird assumption and use a paired sample t-test, with the idea that we're analyzing whether the objective day of events is describesd differently by\n",
    "Fox and MSNBC, which we're conceptualizing as the condition. We're further going\n",
    "to treat what is obviously a fractional variable as a continuous variable, just\n",
    "for sake of implementation and such. Definitely different (slash better) ways \n",
    "one could do this analysis!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 902
    },
    "colab_type": "code",
    "id": "ZvddsyFCnL64",
    "outputId": "f5d24e31-a8a9-4024-953e-3841c477b1b4"
   },
   "outputs": [],
   "source": [
    "sb.distplot(msnbc_pur, label='MSNBC', color='skyblue')\n",
    "sb.distplot(fox_pur, label='Fox', color='crimson')\n",
    "plt.legend()\n",
    "plt.xlabel('Count of purity words')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Do Fox and MSNBC use different amount of purity language?')\n",
    "plt.show()\n",
    "print(ttest_rel(msnbc_pur, fox_pur))\n",
    "\n",
    "sb.distplot(msnbc_auth, label='MSNBC', color='skyblue')\n",
    "sb.distplot(fox_auth, label='Fox', color='crimson')\n",
    "plt.legend()\n",
    "plt.xlabel('Count of authority words')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Do Fox and MSNBC use different amount of authority language?')\n",
    "plt.show()\n",
    "print(ttest_rel(msnbc_auth, fox_auth))\n",
    "\n",
    "sb.distplot(msnbc_ingrp, label='MSNBC', color='skyblue')\n",
    "sb.distplot(fox_ingrp, label='Fox', color='crimson')\n",
    "plt.legend()\n",
    "plt.xlabel('Count of ingroup words')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Do Fox and MSNBC use different amount of ingroup language?')\n",
    "plt.show()\n",
    "print(ttest_rel(msnbc_ingrp, fox_ingrp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jgyJHvJa46Ve"
   },
   "source": [
    "#**Your turn!**"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of First_Workshop_2019.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
