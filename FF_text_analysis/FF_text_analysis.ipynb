{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eOO5SuwcZhIh"
   },
   "source": [
    "# Text Analysis with Scikit-learn\n",
    "\n",
    "### Info\n",
    "\n",
    "- Scott Bailey - Stanford Libraries\n",
    "\n",
    "With great thanks to Javier de la Rosa and Peter Broadwell, who wrote chunks of this workshop during prevous iterations. \n",
    "\n",
    "### What are we covering today?\n",
    "\n",
    "- What do we mean by text analysis?\n",
    "- What is scikit-learn and why would we use it?\n",
    "- Where does scikit-learn fit into a full text analysis pipeline?\n",
    "- Turning texts into numbers\n",
    "- The scikit-learn API\n",
    "- Text classification\n",
    "- Building a pipeline to test multiple algorithms\n",
    "\n",
    "### Requirements\n",
    "\n",
    "If running this notebook locally, please install the next packages before continuing.\n",
    "\n",
    "- `nltk`\n",
    "- `jupyter`\n",
    "- `scikit-learn`\n",
    "- `lime`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-6GVqfnSb1Zk"
   },
   "source": [
    "## Goals\n",
    "\n",
    "By the end of the workshop, we hope you'll have a solid sense of how and why we turn text into numbers, a sense of the scikit-learn standard API, and knowledge of building a pipeline in scikit-learn for text classification. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3Uv0Owz9eI78"
   },
   "source": [
    "## A typical ML classification workflow\n",
    "Generally, the workflow for any classification task is usually as follows: \n",
    "1. **Collect** or create **labeled data**\n",
    "2. **Transform** that data into a numeric representation\n",
    "  - Each numeric value representing a characteristic of the data is called a **feature**\n",
    "  - The set of all features representing a single pair of input data and labels is called the **feature vector**\n",
    "  - The whole labeled data set is split into two parts (at least) to train, evaluate and refine the model: a **training set** and a **test set**\n",
    "3. **Train** (learn/fit) a model on a part of the transformed labeled data (the training set)\n",
    "4. **Test** the model predictions on the test set to evaluate its performance\n",
    "5. **Assess** your model and revisit each of the previous steps, if necessary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xI9wZvu6Q9gZ"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-5qXRRYreC5g"
   },
   "source": [
    "## `scikit-learn`\n",
    "\n",
    "In Python, one solid choice for machine learning is the library [`scikit-learn`](http://scikit-learn.org/stable/):\n",
    "- Simple and efficient tools for data mining and data analysis\n",
    "- Open source, commercially usable (BSD license), and reusable in various contexts\n",
    "- Built on other popular Python libraries:\n",
    "  - NumPy (core numerical processing tools and data structures)\n",
    "  - SciPy (scientific computing functions, including clustering algorithms)\n",
    "  - Matplotlib (plotting and data visualization, intended to resemble MATLAB's viz features, but free)\n",
    "\n",
    "<figure>\n",
    "  <img  src=\"http://scikit-learn.org/stable/_static/ml_map.png\" width=\"75%\" />  <figcaption><div align=\"left\" style=\"padding-top: 4px;\">Source: <a href=\"http://scikit-learn.org/stable/tutorial/machine_learning_map/\">`scikit-learn`: Choosing the right estimator</a></div></figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4MYpxi9YRjWZ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EUL1-d28eP5Z"
   },
   "source": [
    "### scikit-learn's simple and unified API\n",
    "`scikit-learn` provides classes for most of the central machine learning tasks and methods, including those in the classification workflow above. These classes share many of the same interface points, with the goal of making it easier to swap or chain algorithms.\n",
    "\n",
    "For example, all `Estimators` (classes containing the actual learning algorithms)  provide the following interfaces:\n",
    "- `fit()`: load (labeled) input data and compute/\"learn\" various qualities (parameters) of the data\n",
    "- `predict()`: make predictions/inferences about other data after training\n",
    "\n",
    "Some `Estimators` and all feature extractors/`Vectorizers` also provide a `transform()` interface, which modifies and outputs data based on the parameters learned by running `fit()` on the data. The interface `fit_transform()` runs both of these steps on the same input data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5Fdf5sZ92NZ9"
   },
   "source": [
    "## Text classification\n",
    "\n",
    "\n",
    "Given the specific task of assigning a category to a new text based on a set of labeled input texts:\n",
    "1. **Collect and label.** In the case of Twitter data, for example, you'd download a bunch of tweets using the Twitter API, extract their texts from the JSON format of the API response, and then assign one or more labels to each tweet (the easiest way is just to use the tweet's hashtags as labels).\n",
    "2. **Transform.** There are many strategies for turning your textual data into numbers, and `scikit-learn` has built-in libraries for most of them. Usually you'll begin by making a list of the words that apear in a document (a \"bag of words\"), but probably you'll also want to count the *frequencies* of the words in each document (BoW counts). In `scikit-learn`, this is done by a type of transformer called a `CountVectorizer`. We also must  choose whether to exclude uncommon words (i.e., words that only appear in a few documents) or very common words (\"stopwords\"). These high-level settings as a whole are called **hyperparameters** (different from **parameters**, which are the values learned by the model from the training data that enable it to make predictions).\n",
    "3. **Train.** You'll need to choose which learning model/algorithm to use, either by reading the documentation or talking to your friendly neighborhood data scientist. After choosing a model, we train it on part of the labeled input data (the training set).\n",
    "4. **Test.** We then use the trained model to predict/infer the labels of the remainder of the labeled input data (the test/validation set).\n",
    "5. **Assess.** Apply one or more metrics (scores) to evaluate how well the predicted labels match the actual labels of the test/validation set. If the performance is unsatisfactory, we'll need to backtrack, possibly all the way to step #1, getting more labeled data and applying different transformations/hyper-parameters as needed, and/or trying a different model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NK07PRu02fP3"
   },
   "source": [
    "We'll begin by installing any Python libraries we need that aren't already available in Google Colab. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 493
    },
    "colab_type": "code",
    "id": "CthcU3z-eDck",
    "outputId": "255e73ed-f312-48d2-a3da-85e3161d9099"
   },
   "outputs": [],
   "source": [
    "!pip install lime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mg7aZbMAMJpa"
   },
   "outputs": [],
   "source": [
    "# For the basic documentation on text feature extraction,\n",
    "# see https://scikit-learn.org/stable/modules/feature_extraction.html#text-feature-extraction\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "H2Gyda4QMPkv"
   },
   "outputs": [],
   "source": [
    "documents = [\n",
    "    'This is the first document.',\n",
    "     'This document is the second document.',\n",
    "     'And this is the third one.',\n",
    "     'Is this the first document?',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 203
    },
    "colab_type": "code",
    "id": "8IesshTkMRcZ",
    "outputId": "f09b30cc-b6fe-4fdc-b2b9-d3003eb647c5"
   },
   "outputs": [],
   "source": [
    "count_vectorizer = CountVectorizer()\n",
    "count_vectorizer.fit(documents)\n",
    "print(\"Vocabulary size:\", len(count_vectorizer.vocabulary_))\n",
    "count_vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 464
    },
    "colab_type": "code",
    "id": "gbqYn7g6MTuc",
    "outputId": "1d848654-f040-4fec-fd00-48423d945d6d"
   },
   "outputs": [],
   "source": [
    "counts = count_vectorizer.transform(documents)\n",
    "print(counts)\n",
    "print(\"   ^  ^         ^\\n   |  |         |\\n  doc word_id count\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 185
    },
    "colab_type": "code",
    "id": "UWpo_8DOM2ux",
    "outputId": "909e9590-7470-44ef-eea9-3884aad023ca"
   },
   "outputs": [],
   "source": [
    "# This will go through the entire vocabulary, but only show counts from the first doc\n",
    "doc = 0\n",
    "for word, word_id in count_vectorizer.vocabulary_.items():\n",
    "    print(word, \":\", counts[doc, word_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 408
    },
    "colab_type": "code",
    "id": "GbOWJ9SAM5Gl",
    "outputId": "86308b7d-00a6-4687-b7a4-7bf2e1c5223d"
   },
   "outputs": [],
   "source": [
    "# We previously used both the fit and transform methods.\n",
    "# Vectorizers typically have a single fit_transform method we can use to do both\n",
    "# in one step.\n",
    "counts = count_vectorizer.fit_transform(documents)\n",
    "print(counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BoCAijoTM-IB"
   },
   "source": [
    "`CountVectorizer` also has some options to disregard stopwords, count ngrams (multiple adjacent words) instead of single words, cap the maximum number of words in each bag, normalize spelling, or count terms within a frequency range. It is worth exploring the [documentation](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SRU2Q5BaBwpq"
   },
   "source": [
    "### Activity\n",
    "\n",
    "Fill in the text in the code cell below so after transforming it using a `CountVectorizer`, the counts are as shown (order of words is not important).\n",
    "\n",
    "```\n",
    "flowers : 1\n",
    "garden : 1\n",
    "up : 1\n",
    "some : 1\n",
    "the : 1\n",
    "morning : 1\n",
    "place : 1\n",
    "every : 1\n",
    "pick : 1\n",
    "from : 1\n",
    "my : 1\n",
    "by : 1\n",
    "```\n",
    "\n",
    "**Hint**: No special parameters are needed to get this output.\n",
    "\n",
    "**Stretch goal**: What word is missing from the token counts? How would you figure out why it's missing, and how to get it back (if desired)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 323
    },
    "colab_type": "code",
    "id": "5JcyXes7M7DV",
    "outputId": "79355871-d8a2-4518-adb6-8650e24f070f"
   },
   "outputs": [],
   "source": [
    "documents=\"my by from pick every morning by place garden flowers the \"\n",
    "counts = count_vectorizer.fit_transform(documents) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uOdsA2gikDaC"
   },
   "source": [
    "## Activity\n",
    "\n",
    "Take a look at the documentation for the `CountVectorizer`. Create a new vectorizer instance that modifies some of the parameters, such as `ngram_range` or `lowercase`, and run some text of your choice through that vectorizer. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Rh_MmbTOBwpv"
   },
   "source": [
    "## Training and testing\n",
    "\n",
    "`scikit-learn` provides  functions to split a labeled dataset into training and testing sets.\n",
    "\n",
    "**Note**: Many machine learning approaches call for splitting the labeled data into three sets:\n",
    "- **training** data (usually the largest set) for the initial model training\n",
    "- **validation** data, which is then used to evaluate the initial performance of the model and subsequently fine-tune the model settings and **hyperparameters** in the hopes of getting better results\n",
    "- **testing** data is \"held out\" until all model tuning is completed and then is used to give a final evaluation score or *benchmark* of the model's performance.\n",
    "\n",
    "![train test validation split](https://cdn-images-1.medium.com/max/800/1*Nv2NNALuokZEcV6hYEHdGA.png)\n",
    "\n",
    "Train, test, and validation splits<br>Source: Tarang Shah, [About Train, Validation and Test Sets in Machine Learning](https://towardsdatascience.com/train-validation-and-test-sets-72cb40cba9e7)\n",
    "\n",
    "To keep things simple for this tutorial, we'll mostly just use training and test sets.\n",
    "\n",
    "Additionally, in real-world applications, it is highly recommended to split the data set randomly in several different ways (*folds*) and then to compare the performance of the model on the validation/test data across all of these. This approach is called **cross-validation.** The result from a single split can be a fluke or outlier, leading to an unrealistic evaluation of the model. Cross-validation gives us a much clearer picture of the likely performance of the model given arbitrary data, and also can be a way to \"stretch\" the training data when the available training set is small.\n",
    "\n",
    "![4-fold cross validation](https://upload.wikimedia.org/wikipedia/commons/1/1c/K-fold_cross_validation_EN.jpg)\n",
    "\n",
    "4-fold cross validation<br>Source: Wikimedia Commons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AtQaXxtnNafW"
   },
   "source": [
    "## The text corpus\n",
    "\n",
    "For our text classification example, we will be using the [Brown corpus](https://www.nltk.org/book/ch02.html) included in the Natural Language Toolkit (`nltk`), which contains more than a million words of English from 500 texts, where each text is categorized into one of 15 genres. We will consider two of these genre categories: `news` (e.g., the Chicago *Tribune*'s society reportage), and `adventure` (e.g., Peter Field, *Rattlesnake Ridge* (1961)). The goal will be to create a classifier able to assign a text to `news` or `adventure` solely based on its textual contents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5tfrPeOhNKMk"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('brown')\n",
    "from nltk.corpus import brown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XHWxoUoONe_V"
   },
   "outputs": [],
   "source": [
    "for category in brown.categories():\n",
    "    print(category,len(brown.fileids(category)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JHJd_hflOEnV"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "dataset = []\n",
    "categories = ['lore', 'news']\n",
    "new_categories = ['fiction', 'romance']\n",
    "for category in categories:\n",
    "    for fileid in brown.fileids(category):\n",
    "        text = \" \".join(brown.words(fileids=fileid))\n",
    "        dataset.append((text, category))\n",
    "\n",
    "random.shuffle(dataset)\n",
    "print(len(dataset), \"documents:\", \", \".join(\" \".join((str(len(brown.fileids(c))), c)) for c in categories))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xURvR5lrBr9u"
   },
   "outputs": [],
   "source": [
    "#Let's take a quick look at some texts to see what we have.\n",
    "dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "62ud7k6AO4sx"
   },
   "source": [
    "From the dataset, we can now separate the labels and the texts into two different variables. Usually, the variable containing the labels is named `y`, and the one containing the input features (in our case, the texts) is named `X`, as in you can obtain the output `y` as a function of the inputs `X`, which is the core abstraction in `scikit-learn`. But using arbitrary letters is confusing when you're trying to learn a new concept, so we'll add some explanatory info to the variable names after `X_` and `y_`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Jjl0I6qHO2qG"
   },
   "outputs": [],
   "source": [
    "import numpy as np  # scikit-learn works internally with NumPy arrays\n",
    "\n",
    "texts = []\n",
    "labels = []\n",
    "for text, label in dataset:\n",
    "    texts.append(text)\n",
    "    labels.append(label)\n",
    "    \n",
    "X_texts = np.array(texts)\n",
    "y_labels = np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LOnf7bjQO_RL"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "(X_texts_train, X_texts_test,\n",
    " y_labels_train, y_labels_test) = train_test_split(X_texts, y_labels, test_size=0.25, random_state=42)\n",
    "\n",
    "print(\"{} training documents\".format(*X_texts_train.shape))\n",
    "print(\"{} testing documents\".format(*X_texts_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QQ72IHv3PG8R"
   },
   "outputs": [],
   "source": [
    "# Don't forget to transform the text!\n",
    "#vectorizer = CountVectorizer(stop_words='english')\n",
    "vectorizer = CountVectorizer()\n",
    "X_features_train = vectorizer.fit_transform(X_texts_train)\n",
    "X_features_test = vectorizer.transform(X_texts_test)\n",
    "\n",
    "print(\"{} training documents with {} features per document\".format(*X_features_train.shape))\n",
    "print(\"{} testing documents with {} features per document\".format(*X_features_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "geo1wLVGBwqH"
   },
   "source": [
    "## Classification (prediction)\n",
    "\n",
    "Let's start with one of the Naïve Bayes classifiers.\n",
    "\n",
    "**Naïve Bayes** is a family of classifiers based on Bayes' Theorem of probability, which describes the probability of an event based on prior knowledge of possibly relevant conditions. Although its formulation can get confusing, all the math boils down to counting, multiplication and division, making Naïve Bayes (NB) classifiers very fast. On the other hand, NB makes the assumption that all of the features in the data set are equally important and independent, which is obviously not true for words. Despite this, Naïve Bayes classifiers are generally very accurate as text classifiers.\n",
    "\n",
    "<div align=\"left\"><b>\"All models are wrong but some are useful\" - George Box (1978)</b></div>\n",
    "\n",
    "There are three Naïve Bayes algorimths in `scikit-learn`: \n",
    "- Gaussian: assumes that features follow a normal distribution.\n",
    "- Multinomial: good for discrete counts, like in text classification problems using counts of words.\n",
    "- Bernoulli: useful for feature vectors that are binary (i.e. zeros and ones), like classic bag of words.\n",
    "\n",
    "Given that our feature vectors are counts of the words in each document with some additional vocabulary constraints, we will use `MultinomialNB`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AC-JxwSjPJz4"
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "classifier = MultinomialNB()\n",
    "\n",
    "classifier.fit(X_features_train, y_labels_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-F39d7whQfXI"
   },
   "outputs": [],
   "source": [
    "np.shape(classifier.feature_log_prob_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QlvgASU1BwqQ"
   },
   "source": [
    "Now we can predict the categories of previously unseen texts and assess how good our classifier is at classifying them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-OOjqEZyQhDr"
   },
   "outputs": [],
   "source": [
    "samples = [\n",
    "    \"This issue raises new and troubling questions.\",\n",
    "    \"Suddenly the cave entrance collapsed, trapping them inside.\"\n",
    "]\n",
    "transformed_samples = vectorizer.transform(samples)\n",
    "classifier.predict(transformed_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "B_jpMRATBwqV"
   },
   "source": [
    "If we wanted to know which words are being used to decide whether a text is either `news` or `lore`, we need to take into account the the word vocabulary built by the vectorizer we used to transform our data, and the feature (word) probabilities calculated for each label by the model at training time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TG6D6uZLQnfd"
   },
   "outputs": [],
   "source": [
    "def most_informative_features(classifier, vectorizer=None, n=10):\n",
    "    class_labels = classifier.classes_\n",
    "    if vectorizer is None:\n",
    "        feature_names = classifier.steps[0].get_feature_names()\n",
    "    else:\n",
    "        feature_names = vectorizer.get_feature_names()\n",
    "    topn_class1 = sorted(zip(classifier.feature_log_prob_[0], feature_names))[-n:]\n",
    "    topn_class2 = sorted(zip(classifier.feature_log_prob_[1], feature_names))[-n:]\n",
    "    for prob, feat in reversed(topn_class2):\n",
    "        print(class_labels[1], prob, feat)\n",
    "    print()\n",
    "    for prob, feat in reversed(topn_class1):\n",
    "        print(class_labels[0], prob, feat)\n",
    "\n",
    "most_informative_features(classifier, vectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aIHPvGpbQ43Q"
   },
   "outputs": [],
   "source": [
    "%%capture --no-display\n",
    "# Don't worry about this particular chunk of code\n",
    "\n",
    "%matplotlib inline\n",
    "from lime.lime_text import LimeTextExplainer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "def explain(entry, clf, vectorizer=None, n=10):\n",
    "    if vectorizer is None:\n",
    "        class_names = clf.steps[1].classes_.tolist()\n",
    "        pipeline = clf\n",
    "    else:\n",
    "        class_names = clf.classes_.tolist()\n",
    "        pipeline = make_pipeline(vectorizer, clf)\n",
    "    explainer = LimeTextExplainer(class_names=class_names)\n",
    "    exp = explainer.explain_instance(entry, pipeline.predict_proba, num_features=n)\n",
    "    exp.show_in_notebook()\n",
    "\n",
    "explain(\"Reports indicated that the cave entrance collapsed, trapping them inside.\", classifier, vectorizer)\n",
    "# explain(\"They went to a beautiful restaurant, and drank wine together.\", classifier, vectorizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xPceRJmeRsPH"
   },
   "source": [
    "### Activity\n",
    "\n",
    "The list of \"most informative features\" seems to include a lot of really common words, aka \"stopwords.\" The model might perform better if we ignore them. Which code block above would we modify to exclude stopwords from the feature set? (Hint: it's pretty far back). Consulting the documentation might also be helpful. \n",
    "\n",
    "Make this modification and then see how it changes the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NzcLZdoDTmuk"
   },
   "source": [
    "## Model evaluation and selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xphtuyHdvFvQ"
   },
   "source": [
    "Changing model hyper-parameters without really having a way to assess its performance can get problematic. If for example you find better informative features but the classifier is failing 50% of the time, it doesn't really matter much how good you think those features are. In some cases, very performant classifiers make use of unexpected or counterintuitive features.\n",
    "\n",
    "The only real way to assess its performance is by making the classifier predict labels for unseen data, and then comparing the predicted labels with the real labels. This is where the importance of separating training and testing data lies in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kj-Brr5wRr3a"
   },
   "outputs": [],
   "source": [
    "y_pred = classifier.predict(X_features_test)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ccaVzs4cRm4H"
   },
   "outputs": [],
   "source": [
    "print(\"Label     Predicted    Result\\n-----     ---------    ------\")\n",
    "for i, real_label in enumerate(y_labels_test):\n",
    "    predicted_label = y_pred[i]\n",
    "    if real_label == predicted_label:\n",
    "        result =  \"hit\"\n",
    "    else:\n",
    "        result = \"miss\"\n",
    "    print(real_label, \"    \", predicted_label, \"       \", result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "v-0NLXL_6Y67"
   },
   "source": [
    "Here we've ended up with mostly hits, perhaps because of the size of the corpus and the clear differences in the types of texts. We can rerun this with other categories to make sure we get some fails. From there, we can look at how we can understand our hits and misses. Since they're related genres or types, let's pull the `romance` and `fiction` chunks of the corpus. \n",
    "\n",
    "While hit vs. miss gives an idea of what's going on, it doesn't tell us much about the nature of the fails. Were the `romance` texts being classified as `fiction`, or the other way around? This might not seem very important, but if instead of texts you were trying to test whether a person has a deadly desease or not (here labels are positive, has the desease, or negative, does not have the desease), you might be more interested in classifiers with lower false negative ratio.\n",
    "\n",
    "Taking this into account, we could count how many `romance` texts have been correctly classified as `romance` (we will call this `TR`, as in true romance), and how many have been incorrectly classified as `fiction` (`FF`, as in false fiction); and the same for `fiction`: how many `fiction` texts have been classified as `fiction` (`TF`, as in true fiction), and how many as `romance` (`FR`, as in false romance). If we put this into a table, we get a confusion matrix. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "arzrI_edSFRo"
   },
   "source": [
    "![Confusion Matrix c/o Towards Data Science](https://miro.medium.com/max/712/1*Z54JgbS4DUwWSknhDCvNTQ.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "J7zKY0DX3fc_"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "print(\" TF FR\")\n",
    "print(confusion_matrix(y_labels_test, y_pred))\n",
    "print(\" FF TR\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YlIIe-y4_9v7"
   },
   "source": [
    "Let's look at a few other measures of a classifier.\n",
    "\n",
    "**Accuracy** is one of the most important metrics for evaluating classifiers. It's defined as the ratio of correct predictions (\"hits\") to the total number of predictions. We could code it up in a line of Python, but why not just use `scikit-learn`'s [built-in function](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "F7i-42WP8G2h"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accuracy_score(y_labels_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FjjaQzoUARBW"
   },
   "source": [
    "Two other evaluation metrics that often prove useful are *precision* and *recall*. Like accuracy, these can be calculated (averaged) for the entire model, or considered separately for each category. We will use the latter approach here. In this context, the metrics can be defined as follows:\n",
    "\n",
    "- **Precision**: out of the test texts the model classified as fiction, what fraction of them were actually fiction?\n",
    "- **Recall**: out of the total number of texts in the test set, what fraction of them did the model find (i.e., correctly classify as fiction)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ApvwK97eD5KJ"
   },
   "source": [
    "### Activity for post-workshop\n",
    "\n",
    "As an optional activity, use the precision and recall functions provided by `scikit-learn` to evaluate this classifier. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EveY0b6GENfn"
   },
   "source": [
    "We're going to move on to think about how we can automate our classifier build and evaluation to test different hyperparameters for making the model more accurate. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "x3M4S-W3BMXD"
   },
   "outputs": [],
   "source": [
    "def get_accuracy(X, y, vectorizer):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "\n",
    "    X_train = vectorizer.fit_transform(X_train)\n",
    "    X_test = vectorizer.transform(X_test)\n",
    "\n",
    "    clf = MultinomialNB()\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = clf.predict(X_test)\n",
    "\n",
    "    return accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qVRSgIEOOlLs"
   },
   "source": [
    "### Activity\n",
    "\n",
    "`get_accuracy` is a function that takes input data as `X`, its labels as `y`, and a vectorizer. \n",
    "\n",
    "We know that both vocabulary size and stopwords can affect a classifier. Write some code that takes advantage of `get_accuracy` to test out different vocabulary sizes (10000, 1000, 500, 300, 100) and keeping or removing stopwords. Which combination performs best?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "X6YuJz9EPtyQ"
   },
   "source": [
    "We've been looking at the vectorizer or model parameters affecting the accuracy of our classifier, but it could also be the splits that are being used to create training and test data. One common strategy to prevent the split from affecting our model, is to split the dataset randomly several times in different **folds**, calculate whatever measure you want, and then averaging the scores. This process is called **cross-validation**, and prevents us from making the model learn too well the training data (**over-fitting**).\n",
    "\n",
    "In order to cross-validate our model, we need to create a pipeline in `scikit-learn` by combining both the vectorizer and the classifier, then calling the `cross_val_score` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VlyVGqLyPtBy"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "for stopwords in [None, 'english']:\n",
    "    for vocabulary_size in [10000, 1000, 500, 300, 150, 100]:\n",
    "        print(\"stopwords:\", stopwords)\n",
    "        print(\"vocabulary_size:\", vocabulary_size)\n",
    "        vectorizer = CountVectorizer(\n",
    "            max_features=vocabulary_size, stop_words=stopwords)\n",
    "        \n",
    "        pipe = make_pipeline(vectorizer, MultinomialNB())\n",
    "        accuracies = cross_val_score(pipe, X_texts_train, y_labels_train, cv=10)\n",
    "        \n",
    "        print(\"accuracy:\", accuracies.mean())\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "V47of5LLQ_Mg"
   },
   "source": [
    "By looking at this data, which is more reliable, it looks like we could get a classifier with up to ~70% of accuracy. However, if we want a model more intuitive and with a better explanatory power, we could go with the model that removes stop words, although its accuracy is lower."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YGRIzwiAEsKX"
   },
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(max_features=500, stop_words=\"english\")\n",
    "X_train = vectorizer.fit_transform(X_texts_train)\n",
    "X_test = vectorizer.transform(X_texts_test)\n",
    "\n",
    "clf = MultinomialNB()\n",
    "clf.fit(X_train, y_labels_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "most_informative_features(clf, vectorizer, n=10)\n",
    "entry = 0\n",
    "print()\n",
    "\n",
    "explain(\"They told their friend that they found love high atop a mountain in the Sierras.\", clf, vectorizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "x_9Qy6MAZ--f"
   },
   "source": [
    "What if we want to test whether a different classification algorithm might be more effective? Using the pipeline and a simple loop, we can at least start to check this out. You should always be thinking about which algorithms are actually appropriate to your dataset, but when you're prototyping, sometimes you just throw a bunch of algorithms at the problem and see what comes out. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8o_SE_3XZ-S3"
   },
   "outputs": [],
   "source": [
    "# import warnings filter\n",
    "from warnings import simplefilter\n",
    "# ignore all future warnings\n",
    "simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "\n",
    "for clf in (RandomForestClassifier(), LinearSVC(), SVC()):\n",
    "    for stopwords in [None, 'english']:\n",
    "        for vocabulary_size in [10000, 1000, 500, 300, 150, 100]:\n",
    "            print(\"model:\", clf)\n",
    "            print(\"stopwords:\", stopwords)\n",
    "            print(\"vocabulary_size:\", vocabulary_size)\n",
    "            vectorizer = CountVectorizer(\n",
    "                max_features=vocabulary_size, stop_words=stopwords)\n",
    "\n",
    "            pipe = make_pipeline(vectorizer, clf)\n",
    "            accuracies = cross_val_score(pipe, X_texts_train, y_labels_train, cv=10)\n",
    "\n",
    "            print(\"accuracy:\", accuracies.mean())\n",
    "            print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2IeahwoxZZDT"
   },
   "source": [
    "## Other libraries of interest:\n",
    "- SpaCy - NLP - https://spacy.io/\n",
    "- Textacy - NLP + text analysis - https://github.com/chartbeat-labs/textacy\n",
    "- Gensim - semantic modeling - https://radimrehurek.com/gensim/\n",
    "- AllenNLP - https://github.com/allenai/allennlp\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "FF-text-analysis.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
